{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analysing Coastal Change with Landsat\n",
                "\n",
                "This notebook is a simplified, worked example of analysing multiple years of\n",
                "Landsat data to locate the coastline, and its change over time.\n",
                "\n",
                "The original implementation of this algorithm was achieved by Geoscience Australia\n",
                "and is available at [dea-costlines](https://github.com/geoscienceAustralia/dea-coastlines).\n",
                "There is a new version, which is more generic available at\n",
                "[coastlines](https://github.com/auspatious/coastlines).\n",
                "\n",
                "## Requirements\n",
                "\n",
                "The algorithm uses two fundameltal datasets, first, optical Earth observation data,\n",
                "and Landsat is ideal, as it goes back over forty years. And second, is a tidal\n",
                "model, which is used to annotate scenes with a tide height, and to filter\n",
                "those scenes to just those in the middle of the tide range, therefore establishing\n",
                "an 'average' coastline."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration and setup\n",
                "\n",
                "First, we import libraries and tools that we need to run the analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "from coastlines.utils import extract_contours, tide_cutoffs\n",
                "from coastlines.vector import annual_movements, calculate_regressions\n",
                "from dask.distributed import Client\n",
                "from datacube import Datacube\n",
                "from datacube.utils.masking import create_mask_value, valid_data_mask\n",
                "from dea_tools.spatial import points_on_line\n",
                "from eo_tides import pixel_tides\n",
                "from ipyleaflet import basemaps\n",
                "from odc.geo.geom import point\n",
                "from odc.stac import configure_s3_access"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set up our environment\n",
                "\n",
                "We're going to access Landsat data from USGS using the Element-84 STAC API.\n",
                "\n",
                "The `configure_s3_access` function will set up the environment for the requester\n",
                "pays bucket on S3, which USGS shares data from. And we use Dask to lazy-load\n",
                "data and run the computation in parallel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tide data and config\n",
                "tide_data_location = \"~/data/coastlines/tide_models\"\n",
                "\n",
                "# Configure AWS\n",
                "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
                "\n",
                "if \"AWS_NO_SIGN_REQUEST\" in os.environ:\n",
                "    del os.environ[\"AWS_NO_SIGN_REQUEST\"]\n",
                "\n",
                "configure_s3_access(requester_pays=True)\n",
                "\n",
                "# Connect to the Datacube\n",
                "dc = Datacube(app=\"coastlines\")\n",
                "\n",
                "# Set up a dask client\n",
                "dask_client = Client(n_workers=4, threads_per_worker=8)\n",
                "dask_client"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set up a study location\n",
                "\n",
                "Configure a spatial location and a time range."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_date = \"2010\"\n",
                "end_date = \"2025\"\n",
                "max_cloud_cover = 90  # Maximum cloud cover percentage\n",
                "\n",
                "# Find a location you're interested in on Google Maps and copy the coordinates\n",
                "# by right-clicking on the map and clicking the coordinates\n",
                "\n",
                "coords = -5.7417, 106.6088\n",
                "\n",
                "aoi_point = point(coords[1], coords[0], crs=\"EPSG:4326\")\n",
                "area = aoi_point.buffer(0.015).boundingbox\n",
                "\n",
                "area.explore(tiles=basemaps.Esri.WorldImagery)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Find and load data\n",
                "\n",
                "Search for Landsat scenes and load them into memory using Dask."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categories_to_mask_landsat = {\n",
                "    \"cloud\": \"high_confidence\",\n",
                "    \"cloud_shadow\": \"high_confidence\",\n",
                "}\n",
                "\n",
                "# Find and load Landsat datasets\n",
                "landsat_datasets = dc.find_datasets(\n",
                "    product=[\"ls5_c2l2_sr\", \"ls7_c2l2_sr\", \"ls8_c2l2_sr\", \"ls9_c2l2_sr\"],\n",
                "    time=(start_date, end_date),\n",
                "    longitude=(area.left, area.right),\n",
                "    latitude=(area.bottom, area.top),\n",
                "    cloud_cover=(0, max_cloud_cover),\n",
                "    collection_category=\"T1\",  # Only include T1 quality data\n",
                ")\n",
                "\n",
                "print(f\"Found {len(landsat_datasets)} Landsat datasets\")\n",
                "\n",
                "data = dc.load(\n",
                "    datasets=landsat_datasets,\n",
                "    longitude=(area.left, area.right),\n",
                "    latitude=(area.bottom, area.top),\n",
                "    resolution=30,\n",
                "    output_crs=\"EPSG:6933\",\n",
                "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"qa_pixel\"],\n",
                "    group_by=\"solar_day\",\n",
                "    dask_chunks={\"time\": 1, \"x\": 520, \"y\": 520},\n",
                "    resampling={\n",
                "        \"*\": \"cubic\",\n",
                "        \"qa_pixel\": \"nearest\",\n",
                "    },\n",
                "    # skip_broken_datasets=True,\n",
                "    driver=\"rio\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preview data\n",
                "\n",
                "This cell first selects the first four scenes using the `isel` or \"index select\" function.\n",
                "And then plots that from an array, so that we get an RGB visualisation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "subset = data[[\"red\", \"green\", \"blue\"]].isel(time=slice(0, 4))\n",
                "subset.to_array().plot.imshow(col=\"time\", col_wrap=2, size=6, robust=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data processing\n",
                "\n",
                "Now we know we have some good data, we can start processing. First we need to mask\n",
                "out clouds, so that they don't impact our results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mask Landsat data\n",
                "mask_value, _ = create_mask_value(\n",
                "    landsat_datasets[0].product.measurements[\"qa_pixel\"].flags_definition,\n",
                "    **categories_to_mask_landsat,\n",
                ")\n",
                "\n",
                "cloud_mask = (data.qa_pixel & mask_value) != 0\n",
                "valid_data = valid_data_mask(data)\n",
                "mask = cloud_mask | ~valid_data\n",
                "\n",
                "masked = data.where(~mask).drop_vars(\"qa_pixel\")\n",
                "\n",
                "# Scale Landsat data, so that values are between 0 and 1\n",
                "masked = (masked * 0.0000275 - 0.2).clip(0, 1)\n",
                "\n",
                "masked"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview the masked data\n",
                "masked_subset = masked[[\"red\", \"green\", \"blue\"]].isel(time=range(0, 4))\n",
                "masked_subset.to_array().plot.imshow(col=\"time\", col_wrap=2, size=6, robust=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tide masking\n",
                "\n",
                "This cell filters out scenes that are wholy in the \"extreme\" tides, or those\n",
                "outside the middle 50% of observations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add tide height to the data\n",
                "tides = pixel_tides(\n",
                "    masked,\n",
                "    resample=True,\n",
                "    directory=tide_data_location,\n",
                "    model=\"FES2022\",\n",
                "    dask_compute=True,\n",
                ")\n",
                "\n",
                "# Determine tide cutoff\n",
                "tide_cutoff_min, tide_cutoff_max = tide_cutoffs(data, tides, tide_centre=0.0)\n",
                "\n",
                "tide_bool = (tides >= tide_cutoff_min) & (tides <= tide_cutoff_max)\n",
                "data_filtered = masked.sel(time=tide_bool.sum(dim=[\"x\", \"y\"]) > 0)\n",
                "\n",
                "# Apply mask, and load in corresponding tide masked data\n",
                "data_tide_masked = data_filtered.where(tide_bool)\n",
                "\n",
                "data_tide_masked"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Identify land and water\n",
                "\n",
                "We use a water index, here it's a combination of the normalised difference wetness index, NDWI,\n",
                "and the modified version of that, MNDWI. We find the average of the two indices, which has been\n",
                "found to be more robust to issues of noisy data over ocean in Landsat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Combined index (the mean of MNDWI and NDWI)\n",
                "# index_name = \"combined\"\n",
                "# data_tide_masked[\"mndwi\"] = (data_tide_masked.green - data_tide_masked.swir16) / (data_tide_masked.green + data_tide_masked.swir16)\n",
                "# data_tide_masked[\"ndwi\"] = (data_tide_masked.green - data_tide_masked.nir08) / (data_tide_masked.green + data_tide_masked.nir08)\n",
                "# data_tide_masked[\"combined\"] = (data_tide_masked.mndwi + data_tide_masked.ndwi) / 2\n",
                "\n",
                "# # Create SCOWI index, which is an alternative to the above combined index\n",
                "# index_name = \"scowi\"\n",
                "# d = data_tide_masked\n",
                "# data_tide_masked[\"scowi\"] = d.blue + 2 * (d.green - d.nir08) - 0.75 * (d.swir16) - 0.5 * (d.swir16)\n",
                "\n",
                "# # Create MNDWI_NIR index\n",
                "index_name = \"mndwi_nir\"\n",
                "scaled_green = (\n",
                "    data_tide_masked.green\n",
                "    + (data_tide_masked.nir08.max(dim=\"time\") - data_tide_masked.nir08)\n",
                ") / 2\n",
                "scaled_swir1 = (data_tide_masked.swir16 + data_tide_masked.nir08) / 2\n",
                "data_tide_masked[\"mndwi_nir\"] = (scaled_green - scaled_swir1) / (\n",
                "    scaled_green + scaled_swir1\n",
                ")\n",
                "\n",
                "grouped_by_year = (\n",
                "    data_tide_masked[index_name]\n",
                "    .groupby(\"time.year\")\n",
                "    .median()\n",
                "    .to_dataset(name=index_name)\n",
                ")\n",
                "grouped_by_year = grouped_by_year.compute()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grouped_by_year[index_name].plot.imshow(\n",
                "    col=\"year\", col_wrap=2, size=6, cmap=\"RdBu\", robust=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Extract contours\n",
                "\n",
                "Next we extract contour lines from the underlying land/water raster data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You can also try using the other index above\n",
                "# use z_value of 0.0 for 'mndwi'\n",
                "# use z_value of 0.65 for `mndwi_nir`\n",
                "z_values = 0.65\n",
                "\n",
                "contour_gdf = extract_contours(\n",
                "    grouped_by_year, z_values=z_values, index_name=index_name\n",
                ")\n",
                "\n",
                "contour_gdf.reset_index().explore(\n",
                "    column=\"year\",\n",
                "    cmap=\"magma\",\n",
                "    style_kwds={\"weight\": 3},\n",
                "    tiles=basemaps.Esri.WorldImagery,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Extract points\n",
                "\n",
                "And from the contours, we extract points, and annotate them with change over time, so that\n",
                "we can document how much the coastline has eroded or accreted."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract points at every 30 metres along the most recent shoreline\n",
                "points_gdf = points_on_line(contour_gdf, index=2023, distance=30)\n",
                "\n",
                "# Calculate annual movements based on the points from above\n",
                "points_gdf = annual_movements(\n",
                "    points_gdf,\n",
                "    contours_gdf=contour_gdf,\n",
                "    yearly_ds=grouped_by_year,\n",
                "    baseline_year=2023,\n",
                "    water_index=index_name,\n",
                ")\n",
                "\n",
                "# And regression lines\n",
                "points_gdf = calculate_regressions(points_gdf=points_gdf)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualisation\n",
                "\n",
                "Finally, visualise the results together."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add human-friendly label for plotting\n",
                "points_gdf[\"Coastal change\"] = points_gdf.apply(\n",
                "    lambda x: f\"<h4>This coastline has {'<b>retreated</b>' if x.rate_time < 0 else '<b>grown</b>'} \"\n",
                "    f\"by</br><b>{x.rate_time:.2f} m (±{x.se_time:.1f}) per year</b> since \"\n",
                "    f\"<b>{contour_gdf.index[0]}</b></h4>\",\n",
                "    axis=1,\n",
                ")\n",
                "points_gdf.loc[points_gdf.sig_time > 0.05, \"Coastal change\"] = (\n",
                "    f\"<h4>No significant trend of retreat or growth)</h4>\"\n",
                ")\n",
                "\n",
                "m = contour_gdf.reset_index().explore(\n",
                "    column=\"year\",\n",
                "    cmap=\"inferno\",\n",
                "    tooltip=False,\n",
                "    style_kwds={\"opacity\": 0.5},\n",
                "    categorical=True,\n",
                "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
                "    attr=\"ESRI WorldImagery\",\n",
                ")\n",
                "\n",
                "points_gdf.explore(\n",
                "    m=m,\n",
                "    column=\"rate_time\",\n",
                "    cmap=\"RdBu\",\n",
                "    markersize=5,\n",
                "    tooltip=\"Coastal change\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Export results to GeoPackage\n",
                "# location = \"thousand_islands\"\n",
                "# points_gdf.to_file(f\"{location}_rate_of_change.gpkg\", layer=\"rate_of_change\", driver=\"GPKG\")\n",
                "# contour_gdf.to_file(f\"{location}_shorlines.gpkg\", layer=\"shorelines\", driver=\"GPKG\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Explore a full timeseries\n",
                "\n",
                "Have a look at the [full map of results here](https://map.asia.easi-eo.solutions).\n",
                "\n",
                "Click \"explore map data\" and then add \"Indonesian Coastlines\", which contains five sites across Indonesia."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
